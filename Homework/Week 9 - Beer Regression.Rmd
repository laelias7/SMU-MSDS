---
title: "Colorado & Texas Beer"
author: "Linda Eliasen"
date: "March 11, 2019"
output: html_document
---
*Personal SMU Repository: <https://github.com/laelias7/SMU-MSDS.git>*      

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, tidy=TRUE)
```


```{r libraries, include=FALSE}
library(plotly)
library(caret)
library(broom)
library(xtable)
library(cowplot)
library(magrittr)
library(dplyr)
library(tidyr)
library(knitr)
library(readr)
library(ggplot2)
library(ggthemes)
library(ggpubr)
library(stringr)
library(kableExtra)
library(dataMaid)
library(stats)
library(MASS)
```
### Background

Brewmeisters in Colorado and Texas have teamed up to analyze the relationship between ABV and IBU in each of their states.  

Their three main questions of interest are:<br>
     1) Is there a significant linear relationship between ABV (response) and IBU (explanatory)<br>
     2) Is this relationship different between beers in Colorado and Texas<br>
     3) Is there a significant quadratic component in this relationship for either Colorado or Texas or both?<br> 
 <br> 
 
###Clean and prepare the data
Read in data files and merge based on brewery id
```{r}
brew<- read.csv("breweries.csv" , header=TRUE, stringsAsFactors=FALSE)
colnames(brew)<- c('Brewery_id', 'Name', 'City', 'State')

beer <- read.csv("beers.csv" , header=TRUE, stringsAsFactors=FALSE)

brew<-merge(brew,beer, by="Brewery_id",all.x = TRUE)
colnames(brew)<- c('Brewery_id', 'Brewery', 'City', 'State', 'Beer', 'Beer_ID', 'ABV', 'IBU', 'Style', 'Ounces')
brew$IBU <- as.numeric(brew$IBU)
```

Format and create new file with Colorado and Texas data. Show the first few rows of the data.
```{r}
#need to remove extra spaces from state abreviation
brew %<>% mutate(State = gsub(' ', '', State))

#Pullling CO and TX from the data
co<-brew[grep("CO", brew$State),]
tx<-brew[grep("TX", brew$State),]

#Final data set with CO and TX; removing NAs and reordering
beerCOTX<-rbind(co,tx)
beerCOTX<-beerCOTX[-which(is.na(beerCOTX$IBU)),]
beerCOTX = beerCOTX[order(beerCOTX$IBU),]
#check file
kable(head(beerCOTX,6))%>%
  kable_styling(bootstrap_options = "striped", "condensed")

```

###Create initial plots of the data 
<br>
Plot ABV v. IBU for both Colorado and Texas.  

```{r}
p <- ggplot(data = beerCOTX, aes(x = IBU, y = ABV)) + geom_point()
p + facet_wrap(~State)

```

###Model the data

For each state, fit a simple linear regression model to assess the relationship between ABV and IBU. 
Use the regular plot function in base R to create a scatter plot with the regression line superimposed on the plot.

####Colorado 
<br>
The data exhibits a positive linear relationship, as the IBU level of beer increases, so does the alcohol by volume.  Based on the regression results, we see that IBU accounts for roughly 44% of the variance in ABV levels.
```{r}
sco<- filter(beerCOTX, State == "CO")

fit = lm(ABV~IBU,data = sco) 
fit.res = resid(fit)

  # Scatterplot
plot(sco$IBU, sco$ABV, main="CO ABV vs. IBU", 
   xlab="IBU", ylab="ABV", pch=16)  
# Add fit lines
abline(lm(sco$ABV~sco$IBU), col="red") 

summary(fit)
```

####Texas
<br>
Although there is some clustering towards the lower end of the graph, We see a positive linear relationship.  As the IBU increases, so does the alcohol by volume.  Based on the regression results, we see that IBU accounts for 59% of the variance in ABV levels.
```{r}
stx<- filter(beerCOTX, State == "TX")

fit2 = lm(ABV~IBU,data = stx) 
fit.res2 = resid(fit2)

  # Scatterplot
plot(stx$IBU, stx$ABV, main="TX ABV vs. IBU", 
   xlab="IBU", ylab="ABV", pch=16)  
# Add fit lines
abline(lm(stx$ABV~stx$IBU), col="red") 

summary(fit2)
```

###Address the assumptions of the regression model

####Colorado
<br>
The relationship between the two variables appears to be linear.  Generally, as IBU increases so does ABV.  
<br>
In the residuals versus predicted plot, the residuals are randomly scattered around the center line of zero, with no obvious pattern. There also appears to be equal variance of the errors.  Note that there are three points that may be evidence of outliers, but they don't seem to have a noticeable impact.
<br>
The histogram of residuals and qq plot are approximately normally distributed.  Note that there is slight curvature in the qqplot, but it does not create a significant departure from normality.

```{r}
#residual pattern only
resfit<-resid(fit)
plot(sco$IBU, resfit, ylab = "Residual", xlab="Predicted Value", main = "Residual Plot")
abline(0, 0) 

 #residual graph for equal variance
 res<-ggplot(fit, aes(.fitted, .resid))+
  geom_point(aes(color = .resid)) +
  scale_color_gradient2(low = "blue", 
                        mid = "white",
                        high = "red") +
  guides(color = FALSE)
res<-res+
  stat_smooth(method="lm",se=F)+
  geom_hline(yintercept=0, 
             col="red", 
             linetype="dashed")
res<-res+
  xlab("Predicted Value")+
  ylab("Residuals")
res<-res+theme_classic()
res+geom_segment(aes(y=0,x=.fitted,xend=.fitted, yend=.resid,alpha = 2*abs(.resid)))+guides(alpha=FALSE)


 #QQPlot for standardized residuals
 ggplot(fit)+
   stat_qq(aes(sample=.stdresid))+
   geom_abline()

#Histogram
 sresid <- studres(fit) 
hist(sresid, freq=FALSE, 
   main="Distribution of Studentized Residuals")
xfit<-seq(min(sresid),max(sresid),length=40) 
yfit<-dnorm(xfit) 
lines(xfit, yfit)
```

####Texas
<br>
The relationship between the two variables appears to be linear.  Generally, as IBU increases so does ABV.  However, we do see slight clustering at the lower levels.
<br>
In the residuals versus predicted plot, the residuals are randomly scattered around the center line of zero, with no obvious pattern. There also appears to be equal variance of the errors.  Again, we have to be mindful of the slight clustering and a couple of potential outliers.
<br>
The histogram of residuals and qq plot are approximately normally distributed, although we do note that there is mild right skewness.
<br>
```{r}
#residual pattern only
resfit2<-resid(fit2)
plot(stx$IBU, resfit2, ylab = "Residual", xlab="Predicted Value", main = "Residual Plot")
abline(0, 0) 

 #residual graph for equal variance
 res2<-ggplot(fit2, aes(.fitted, .resid))+
  geom_point(aes(color = .resid)) +
  scale_color_gradient2(low = "blue", 
                        mid = "white",
                        high = "red") +
  guides(color = FALSE)
res2<-res2+
  stat_smooth(method="lm",se=F)+
  geom_hline(yintercept=0, 
             col="red", 
             linetype="dashed")
res2<-res2+
  xlab("Predicted Value")+
  ylab("Residuals")
res2<-res2+theme_classic()
res2+geom_segment(aes(y=0,x=.fitted,xend=.fitted, yend=.resid,alpha = 2*abs(.resid)))+guides(alpha=FALSE)


 #QQPlot for standardized residuals
 ggplot(fit2)+
   stat_qq(aes(sample=.stdresid))+
   geom_abline()

#Histogram
 sresid <- studres(fit2) 
hist(sresid, freq=FALSE, 
   main="Distribution of Studentized Residuals")
xfit2<-seq(min(sresid),max(sresid),length=40) 
yfit2<-dnorm(xfit2) 
lines(xfit2, yfit2)
```


###Gain inference from the model

####Interpret the slope of the regression model.  

####Colorado
<br>
<br>
Predicted ABV =0.0474013 + 0.0003676(IBU).  For every one unit increase in IBU, ABV will increase by 0.037%.
<br>
We are 95% confident that for every one unit increase in IBU, the ABV increase will be between 0.030% and 0.044% 

```{r}
  fit.table <- xtable(fit)
  kable(fit.table, caption = "CO Model 1:  ABV = B0+B1(IBU)")%>%
kable_styling(bootstrap_options = "striped", "condensed")

 #Conf Interval alpha = .05
cin.table<-xtable(confint(fit))
kable(cin.table, caption = "95% Confidence Interval")%>%
kable_styling(bootstrap_options = "striped", "condensed")
```

####Texas
<br>
<br>
Predicted ABV =0.0434737 + 0.0004172(IBU).  For every one unit increase in IBU, ABV will increase by 0.042%.
<br>
We are 95% confident that for every one unit increase in IBU, the ABV increase will be between 0.034% and 0.049% 
```{r}
  fit.table2 <- xtable(fit2)
  kable(fit.table2, caption = "Model 1:  ABV = B0+B1(IBU)")%>%
kable_styling(bootstrap_options = "striped", "condensed")

  #Conf Interval alpha = .05
cin.table2<-xtable(confint(fit2))
kable(cin.table2, caption = "95% Confidence Interval")%>%
kable_styling(bootstrap_options = "striped", "condensed")
```

####Is there evidence that the relationship between ABV and IBU is significantly different for Texas and Colorado beers?  
<br>
Based on the parameter estimates for IBU, Texas may have a slightly larger impact, but we don't know for sure.
<br>
Now that we know the confidence intervals for the slope, we have evidence that Texas has a plausible range of (0.0003440, 0.0004904) as opposed to Colorado with (0.0002997, 0.0004354). Based on this, we can say with a greater degree of confidence that for beers produced in Texas the relationship between ABV and IBU is signficiantly different.
<br>
Further, based on the linear regression, we know that in Texas, IBU accounts for 60% of the variance in ABV vs. 44% for beers produced in Colorado.
<br>
<br>

###Compare two competing models: External Cross Validation

Using the beerCOTX dataframe, add a column to the data that is the square of the IBU column.  Call it IBU2.  Print the head of the dataframe with the new column.  
```{r}
beerCOTX<-beerCOTX%>% mutate(`IBU2`= IBU^2)

kable(head(beerCOTX,6))%>%
  kable_styling(bootstrap_options = "striped", "condensed")
```
For each state, create a training and test set from the data (60%/40% split respectively).  
<br>
Print a summary of each new data frame. there should be four: TrainingCO, TestCO, TrainingTX, TestTX.  
```{r}
sco2<- filter(beerCOTX, State == "CO")

set.seed(8)
trainIndex<-createDataPartition(sco2$ABV,p=0.6,list=FALSE,times=1)
TrainingCO<-sco2[trainIndex,]
TestCO<-sco2[-trainIndex,]

stx2<- filter(beerCOTX, State == "TX")

set.seed(8)
trainIndex2<-createDataPartition(stx2$ABV,p=0.6,list=FALSE,times=1)
TrainingTX<-stx2[trainIndex2,]
TestTX<-stx2[-trainIndex2,]
```

Summary:  TrainingCO
```{r}
summary(TrainingCO)
```

Summary:  TestCO
```{r}
summary(TestCO)
```

Summary:  TrainingTX
```{r}
summary(TrainingTX)
```

Summary:  TestTX
```{r}
summary(TestTX)
```

###Brewmeisters are curious if the relationship between ABV and IBU is purely linear or if there is evidence of a quadratic component as well.  To test this we would like to compare two models:
<br>
Model 1:  ABV = B0+B1(IBU)<br>
<br>
Model 2:  ABV = B0+B1(IBU)+B2(IBU)^2
<br>

###Colorado
```{r}
#Model1 - Colorado Training Set 
fitTrainCO = lm(ABV~IBU,data = TrainingCO)
predstrainCO = predict(fitTrainCO)
predstestCO = predict(fitTrainCO, newdata = TestCO)

#MSE Model 1 - Colorado Training Set
MSEholderTrainingCO = sum((predstrainCO - TrainingCO$ABV)^2)/(length(TrainingCO$ABV) - 2)

#MSE Model 1 - Colorado Test Set
MSEholderTestCO = sum((predstestCO - TestCO$ABV)^2)/(length(TestCO$ABV) - 2)

#Model 2 - Colorado Training Set
fitTrainCO2 = lm(ABV~IBU + IBU2,data = TrainingCO)
predstrainCO2 = predict(fitTrainCO2)
predstestCO2 = predict(fitTrainCO2, newdata = TestCO)

#MSE Model 2 - Colorado Training Set
MSEholderTrainingCO2 = sum((predstrainCO2 - TrainingCO$ABV)^2)/(length(TrainingCO$ABV) - 2)

#MSE Model 2 - Colorado Test Set
MSEholderTestCO2 = sum((predstestCO2 - TestCO$ABV)^2)/(length(TestCO$ABV) - 2)

```

####Model 1 vs. Model 2 - MSE Comparison - Test Data Colorado
```{r}
Model_1_CO<-MSEholderTestCO

Model_2_CO<-MSEholderTestCO2

MT<-rbind(Model_1_CO, Model_2_CO)
kable(MT)%>%
  kable_styling(bootstrap_options = "striped", "condensed")
```

####Model 1 vs. Model 2 - Adj R-Square Comparison and AIC - Training Data Colorado
```{r}
summary(fitTrainCO)
AIC(fitTrainCO)

summary(fitTrainCO2)
AIC(fitTrainCO2)

```


###Texas
```{r}
#Model1 - Texas Training Set 
fitTrainTX = lm(ABV~IBU,data = TrainingTX)
predstrainTX = predict(fitTrainTX)
predstestTX = predict(fitTrainTX, newdata = TestTX)

#MSE Model 1 - Texas Training Set
MSEholderTrainingTX = sum((predstrainTX - TrainingTX$ABV)^2)/(length(TrainingTX$ABV) - 2)

#MSE Model 1 - Texas Test Set
MSEholderTestTX = sum((predstestTX - TestTX$ABV)^2)/(length(TestTX$ABV) - 2)

#Model 2 - Texas Training Set
fitTrainTX2 = lm(ABV~IBU +IBU2,data = TrainingTX)
predstrainTX2 = predict(fitTrainTX2)
predstestTX2 = predict(fitTrainTX2, newdata = TestTX)

#MSE Model 2 - Texas Training Set
MSEholderTrainingTX2 = sum((predstrainTX2 - TrainingTX$ABV)^2)/(length(TrainingTX$ABV) - 2)

#MSE Model 2 - Texas Test Set
MSEholderTestTX2 = sum((predstestTX2 - TestTX$ABV)^2)/(length(TestTX$ABV) - 2)

```


####Model 1 vs. Model 2 - MSE Comparison - Test Data - Texas
```{r}
Model_1_TX<-MSEholderTestTX

Model_2_TX<-MSEholderTestTX2

MT<-rbind(Model_1_TX, Model_2_TX)
kable(MT)%>%
  kable_styling(bootstrap_options = "striped", "condensed")
```

####Model 1 vs. Model 2 - Adj R-Square Comparison and AIC - Training Data Colorado

```{r}
summary(fitTrainTX)
AIC(fitTrainTX)

summary(fitTrainTX2)
AIC(fitTrainTX2)

```
 

The quadratic model is a linear model of two variables, one of which is the square of the other. When the quadratic model was run on the Colorado traning data, the p-values under that model for IBU and IBU2 were 0.399 and 0.341 respectively. When run on the Texas training data, the p-values under that model for IBU and IBU2 were 0.355 and 0.236 respectively.  In other words, the predictors are not meaningful for the regression, so it is not a good model and should not be used.  

However, we will continue with the comparison of the test data results.  In both Colorado and Texas, the MSE for the quadratic model applied on the test data is not signifiantly higher than the linear model. The linear model is the best choice. <br>
<br>
**MSE Comparison - Test Data** <br>
Colorado <br>
Linear Model:  0.000134 <br>
Quadratic Model:  0.000131 <br>
<br>
Texas <br>
Linear Model:  0<br>
Quadratic Model:  0<br>
<br>
<br>
**Adj R-Squared Comparison - Training Data** <br>
Colorado<br>
Linear Model:  0.4308<br>
Quadratic Model:  0.4303<br>
<br>
Texas<br>
Linear Model:  0.5642<br>
Quadratic Model:  0.5678<br>
<br>
<br>
**AIC Comparison - Training Data** <br>
Colorado<br>
Linear Model:  -555.57<br>
Quadratic Model: -554.52 <br>
<br>
Texas<br>
Linear Model:  -351.05<br>
Quadratic Model:  -350.54<br>
